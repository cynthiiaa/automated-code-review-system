# Production Environment Configuration (CUDA GPU Server)
# Copy to .env for production deployment

# Environment
ENVIRONMENT=production

# Model Configuration
MODEL_PATH=codellama/CodeLlama-7b-hf
MAX_BATCH_SIZE=8  # Higher batch size for CUDA
QUANTIZATION_BITS=4  # Use 4-bit quantization for efficiency
COMPILE_MODEL=true  # Enable torch.compile for performance
ENABLE_FLASH_ATTENTION=true  # Enable on CUDA

# Memory Settings (Optimized for CUDA)
MEMORY_CACHE_SIZE=500
MAX_SEQUENCE_LENGTH=4096  # Longer sequences for production
MAX_NEW_TOKENS=1024

# API Configuration
API_HOST=0.0.0.0  # Listen on all interfaces
API_PORT=8000
API_WORKERS=4  # Multiple workers for production
CORS_ORIGINS=https://yourdomain.com,https://api.yourdomain.com

# Cache Configuration (Production Redis)
REDIS_HOST=redis-server.internal
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=your_redis_password
CACHE_TTL=7200  # 2 hours cache
ENABLE_REDIS=true
ENABLE_MEMORY=true  # Hybrid caching
COMPRESSION=true

# GitHub Integration
GITHUB_TOKEN=your_production_github_token

# Logging and Monitoring
LOG_LEVEL=INFO

# Security (Optional)
# JWT_SECRET=your_jwt_secret_for_auth
# RATE_LIMIT_STORAGE=redis://redis-server.internal:6379

# Performance Tuning
UVICORN_WORKERS=4
UVICORN_WORKER_CLASS=uvicorn.workers.UvicornWorker
UVICORN_TIMEOUT_KEEP_ALIVE=75
UVICORN_MAX_REQUESTS=1000
UVICORN_MAX_REQUESTS_JITTER=100